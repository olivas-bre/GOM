# The Gesture Operational Model (GOM)
This repository provides the code for creating interpretable human motion representations based on state-space modeling.
On this first version is provided a Jupyter notebook in 'Example_ConstantGOM' that illustrate how to compute constant motion representations based on GOM:

![Diagram of GOM's spatial and temporal assumptions](https://user-images.githubusercontent.com/62239010/229471037-da45c7ff-719f-4cec-bafa-d03164782968.PNG)

Soon, new examples for computing time-varying representations using autoencoders will be provided. Check for updates!

