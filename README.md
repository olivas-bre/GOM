# The Gesture Operational Model (GOM)
This repository provides the code for creating interpretable human motion representations based on state-space modeling.

**V1.0** On this first version is provided a Jupyter notebook in `Example_ConstantGOM` that illustrate how to compute constant motion representations based on GOM:

**V1.1** The code for simulating professional movements using autoencoder is provided in `TimeVarying_GOM`. Two Jupyter Notebooks are provided, one illustrates how to compute time-varying GOM representations with a Variational Autoencoder (VAE-RGOM), and the other with an Autoencoder with Global Attention (ATT-RGOM).

![Diagram of GOM's spatial and temporal assumptions](https://user-images.githubusercontent.com/62239010/229471037-da45c7ff-719f-4cec-bafa-d03164782968.PNG)

Soon, new examples for visualizing time-varying representations for dexterity analysis will be provided. Check for updates!

