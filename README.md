# The Gesture Operational Model (GOM)
This repository provides the code for creating interpretable human motion representations based on state-space modeling.
On this first version is provided a Jupyter notebook in 'Example_ConstantGOM' that illustrate how to compute constant motion representations based on GOM:

<p align="center">
  <img src="[http://some_place.com/image.png](https://user-images.githubusercontent.com/62239010/229468878-86fee99a-9d76-4357-98fa-f5cb1ecc5bc0.png)" />
</p>

Soon, new examples for computing time-varying representations using autoencoders will be provided. Check for updates!
